{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45d46e3f",
   "metadata": {},
   "source": [
    "### RAG Piplines: Data Ingestion to Vector DB Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fdbb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='tqdm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385e355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read all the pdf's inside the directory\n",
    "from pathlib import Path\n",
    "def process_all_pdfs(pdf_directory):\n",
    "  \"\"\"Process all PDF files in a directory\"\"\"\n",
    "  all_documents = []\n",
    "  pdf_dir = Path(pdf_directory)\n",
    "\n",
    "  # Find all PDF files recursively\n",
    "  pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "  print(f\"Found {len(pdf_files)} PDF files to process.\")\n",
    "\n",
    "  for pdf_file in pdf_files:\n",
    "    print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "    try:\n",
    "      loader = PyPDFLoader(str(pdf_file))\n",
    "      documents = loader.load()\n",
    "\n",
    "      # Add source information to metadata\n",
    "      for doc in documents:\n",
    "        doc.metadata['source_file'] = pdf_file.name\n",
    "        doc.metadata['file_type'] = 'pdf'\n",
    "\n",
    "        all_documents.extend(documents)\n",
    "        print(f\"Loaded {len(documents)} pages\")\n",
    "\n",
    "    except Exception as e:\n",
    "      print(f\"Error: {e}\")\n",
    "\n",
    "  print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "  return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f0f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df44668",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text splitting get into chunks\n",
    "\n",
    "def split_documents(documents, chunk_size=1000,chunk_overlap=200):\n",
    "  \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "  text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\",\"\\n\",\" \",\"\"]\n",
    "  )\n",
    "  split_docs = text_splitter.split_documents(documents)\n",
    "  print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "\n",
    "  if split_docs:\n",
    "    print(f\"\\nExample chunk:\")\n",
    "    print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "    print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "\n",
    "  return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d652e7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks=split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcca963f",
   "metadata": {},
   "source": [
    "### Step-3: Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cab9c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c81cfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingManager:\n",
    "  \"\"\"\n",
    "    Handles document embedding generation usng SentenceTransformer\n",
    "  \"\"\"\n",
    "  def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    Initialize the embedding manager\n",
    "\n",
    "    Args:\n",
    "      model_name: HuggingFace model name for sentence embeedings\n",
    "    \"\"\"\n",
    "    self.model_name = model_name\n",
    "    self.model = None\n",
    "    self._load_model()\n",
    "  \n",
    "  def _load_model(self):\n",
    "    \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "    try:\n",
    "      print(f\"Loading embedding model: {self.model_name}\")\n",
    "      self.model = SentenceTransformer(self.model_name)\n",
    "      print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "    except Exception as e:\n",
    "      print(f\"Error loading model {self.model_name}: {e}\")\n",
    "      raise\n",
    "\n",
    "  def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of texts\n",
    "\n",
    "    Args: \n",
    "      texts: List of text strings to emded\n",
    "    \n",
    "    Returns:\n",
    "      numpy array of emdeddings with shape (len(texts), embedding_dim)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if not self.model:\n",
    "      raise ValueError(\"Model not loaded\")\n",
    "    \n",
    "    print(f\"Generating emdeddings for {len(texts)} texts....\")\n",
    "    embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "    print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "    return embeddings\n",
    "  \n",
    "  def get_embedding_dimension(self) -> int:\n",
    "    \"\"\"Get the embedding dimension of the model\"\"\"\n",
    "    if not self.model:\n",
    "      raise ValueError(\"Model not loaded.\")\n",
    "    \n",
    "    return self.model.get_sentence_embedding_dimension()\n",
    "  \n",
    "\n",
    "## initialize the embedding manager\n",
    "\n",
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8b856c",
   "metadata": {},
   "source": [
    "### Step 4: VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e23f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "  \"\"\"\n",
    "    Manages document embedding in a ChromaDB vector store\n",
    "  \"\"\"\n",
    "  def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "    \"\"\"\n",
    "    Initialize the vector store\n",
    "\n",
    "    Args:\n",
    "      collection_name: Name of the ChromaDB collection\n",
    "      persist_directory: Directory to persist the vector store\n",
    "    \"\"\"\n",
    "    self.collection_name = collection_name\n",
    "    self.persist_directory = persist_directory\n",
    "    self.client = None\n",
    "    self.collection = None\n",
    "    self._initialize_store()\n",
    "  \n",
    "  def _initialize_store(self):\n",
    "    \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "\n",
    "    try:\n",
    "      # Create persistent ChromaDB client\n",
    "      os.makedirs(self.persist_directory, exist_ok=True)\n",
    "      self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "      # Get or create collection\n",
    "      self.collection = self.client.get_or_create_collection(\n",
    "        name=self.collection_name,\n",
    "        metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "      )\n",
    "      print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "      print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "    except Exception as e:\n",
    "      print(f\"Error initializing vector store: {e}\")\n",
    "      raise\n",
    "  \n",
    "  def add_documents(self, documents: List[Any], embeddings: np.array):\n",
    "    \"\"\"\n",
    "    Add documents and their embeddings to the vector store\n",
    "    \n",
    "    Args:\n",
    "      documents: List of LangChain documents\n",
    "      embeddings: Corresponding embeddings for the documents\n",
    "    \"\"\"\n",
    "\n",
    "    if len(documents) != len(embeddings):\n",
    "      raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "    \n",
    "    print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "\n",
    "    # Prepare data for ChromaDB\n",
    "\n",
    "    ids = []\n",
    "    metadatas = []\n",
    "    documents_text = []\n",
    "    embeddings_list = []\n",
    "\n",
    "    for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "      # Generate unique ID\n",
    "      doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "      ids.append(doc_id)\n",
    "\n",
    "      # Prepare metadata\n",
    "      metadata = dict(doc.metadata)\n",
    "      metadata['doc_index'] = i\n",
    "      metadata['content_length'] = len(doc.page_content)\n",
    "      metadatas.append(metadata)\n",
    "\n",
    "      # Document content\n",
    "      documents_text.append(doc.page_content)\n",
    "\n",
    "      # Embedding\n",
    "\n",
    "      embeddings_list.append(embedding.tolist())\n",
    "    \n",
    "    # Add to collection\n",
    "    try:\n",
    "      self.collection.add(\n",
    "        ids=ids,\n",
    "        embeddings=embeddings_list,\n",
    "        metadatas=metadatas,\n",
    "        documents=documents_text\n",
    "      )\n",
    "      print(f\"Sucessfully added {len(documents)} documents to vector store\")\n",
    "      print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "    except Exception as e:\n",
    "      print(f\"Error adding documents to vector store: {e}\")\n",
    "      raise\n",
    "\n",
    "\n",
    "# Initialize Vector Store\n",
    "vectorstore = VectorStore()\n",
    "vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9009af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert the text to embeddings\n",
    "texts = [doc.page_content for doc in chunks]\n",
    "\n",
    "## Generate the Embeddings\n",
    "\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "## Store in the vector database\n",
    "vectorstore.add_documents(chunks, embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69605c3f",
   "metadata": {},
   "source": [
    "### Step 5: Retriever Pipeline From VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c308ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "  \"\"\"Handles query-based retrival from the vector store\"\"\"\n",
    "\n",
    "  def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "    \"\"\"\n",
    "    Initialize the retriever\n",
    "\n",
    "    Args:\n",
    "      vector_store: Vector store containing document embeddings\n",
    "      embedding_manager: Manager for gernerating query embeddings\n",
    "    \"\"\"\n",
    "    self.vector_store = vector_store\n",
    "    self.embedding_manager = embedding_manager\n",
    "    \n",
    "  def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents for a query\n",
    "\n",
    "    Args:\n",
    "      query: The search query\n",
    "      top_k: Number of top results to return\n",
    "      score_threshold: Minimum similarity score threshold\n",
    "    Returns:\n",
    "      List of dictionaries containing retrieved documents and metadata\n",
    "    \"\"\"\n",
    "    print(f\"Retrieving documents for query: '{query}'\")\n",
    "    print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "\n",
    "    # Generate query embedding\n",
    "    query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "\n",
    "    # Search in vector store\n",
    "\n",
    "    try:\n",
    "      results = self.vector_store.collection.query(\n",
    "        query_embeddings=[query_embedding.tolist()],\n",
    "        n_results=top_k\n",
    "      )\n",
    "\n",
    "      # Processed results\n",
    "      retrieved_docs = []\n",
    "\n",
    "      if results['documents'] and results['documents'][0]:\n",
    "        documents = results['documents'][0]\n",
    "        metadatas = results['metadatas'][0]\n",
    "        distances = results['distances'][0]\n",
    "        ids = results['ids'][0]\n",
    "\n",
    "        for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "          # Convert disance to similarity score (ChromaDB uses cosine distance)\n",
    "          similarity_score = 1 - distance\n",
    "\n",
    "          if similarity_score >= score_threshold:\n",
    "            retrieved_docs.append({\n",
    "              'id': doc_id,\n",
    "              'content': document,\n",
    "              'metadata': metadata,\n",
    "              'similarity_score': similarity_score,\n",
    "              'rank': i + 1\n",
    "            })\n",
    "\n",
    "            print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "          else:\n",
    "            print(f\"No documents found\")\n",
    "        return retrieved_docs\n",
    "    except Exception as e:\n",
    "      print(f\"Error during retrieval: {e}\")\n",
    "      return []\n",
    "    \n",
    "\n",
    "rag_retriever = RAGRetriever(vectorstore, embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44bd978",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0789834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_retriever.retrieve(\"What is Deep learning is all you need?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59306da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9512f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Open AI LLM \n",
    "chat_model = ChatOpenAI(model=\"gpt-4o-mini\", api_key=api_key, temperature=0.1, max_completion_tokens=1024)\n",
    "\n",
    "# Simple RAG function: retrieve context + generate response\n",
    "\n",
    "def rag_simple(query, retriever, llm, top_k=3):\n",
    "  # Retrieve relevant documents\n",
    "  results = retriever.retrieve(query, top_k=top_k)\n",
    "  # Combile retrieved document contents\n",
    "  context = \"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "\n",
    "  if not context:\n",
    "    return \"No relevant context found to answer the question.\"\n",
    "  \n",
    "  # Create prompt\n",
    "  prompt = f\"\"\"Use the following context to answer concisely.\n",
    "               Context: {context}\n",
    "               Question: {query}\n",
    "               Answer:\n",
    "            \"\"\"\n",
    "\n",
    "  # Generate the answer using OpenAI LLM\n",
    "  response = llm.invoke([prompt.format(context=context,query=query)])\n",
    "  return response.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62610343",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer =rag_simple(\"What is deep learning?\", rag_retriever, chat_model)\n",
    "answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf315e24",
   "metadata": {},
   "source": [
    "### Step 6: Enhanched RAG Pipeline  Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b65bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Enhanced RAG Pipeline Features ---\n",
    "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    \"\"\"\n",
    "    RAG pipeline with extra features:\n",
    "    - Returns answer, sources, confidence score, and optionally full context.\n",
    "    \"\"\"\n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return {'answer': 'No relevant context found.', 'sources': [], 'confidence': 0.0, 'context': ''}\n",
    "    \n",
    "    # Prepare context and sources\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'][:300] + '...'\n",
    "    } for doc in results]\n",
    "    confidence = max([doc['similarity_score'] for doc in results])\n",
    "    \n",
    "    # Generate answer\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    \n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "    return output\n",
    "\n",
    "# Example usage:\n",
    "result = rag_advanced(\"What is Deep Learning?\", rag_retriever, chat_model, top_k=3, min_score=0.1, return_context=True)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Sources:\", result['sources'])\n",
    "print(\"Confidence:\", result['confidence'])\n",
    "print(\"Context Preview:\", result['context'][:300])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab815df",
   "metadata": {},
   "source": [
    "### Advanced RAG Pipeline: Streaming, Citations, History, Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd8ca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Advanced RAG Pipeline: Streaming, Citations, History, Summarization ---\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "class AdvancedRAGPipeline:\n",
    "    def __init__(self, retriever, llm):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.history = []  # Store query history\n",
    "\n",
    "    def query(self, question: str, top_k: int = 5, min_score: float = 0.2, stream: bool = False, summarize: bool = False) -> Dict[str, Any]:\n",
    "        # Retrieve relevant documents\n",
    "        results = self.retriever.retrieve(question, top_k=top_k, score_threshold=min_score)\n",
    "        if not results:\n",
    "            answer = \"No relevant context found.\"\n",
    "            sources = []\n",
    "            context = \"\"\n",
    "        else:\n",
    "            context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "            sources = [{\n",
    "                'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "                'page': doc['metadata'].get('page', 'unknown'),\n",
    "                'score': doc['similarity_score'],\n",
    "                'preview': doc['content'][:120] + '...'\n",
    "            } for doc in results]\n",
    "            # Streaming answer simulation\n",
    "            prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
    "            if stream:\n",
    "                print(\"Streaming answer:\")\n",
    "                for i in range(0, len(prompt), 80):\n",
    "                    print(prompt[i:i+80], end='', flush=True)\n",
    "                    time.sleep(0.05)\n",
    "                print()\n",
    "            response = self.llm.invoke([prompt.format(context=context, question=question)])\n",
    "            answer = response.content\n",
    "\n",
    "        # Add citations to answer\n",
    "        citations = [f\"[{i+1}] {src['source']} (page {src['page']})\" for i, src in enumerate(sources)]\n",
    "        answer_with_citations = answer + \"\\n\\nCitations:\\n\" + \"\\n\".join(citations) if citations else answer\n",
    "\n",
    "        # Optionally summarize answer\n",
    "        summary = None\n",
    "        if summarize and answer:\n",
    "            summary_prompt = f\"Summarize the following answer in 2 sentences:\\n{answer}\"\n",
    "            summary_resp = self.llm.invoke([summary_prompt])\n",
    "            summary = summary_resp.content\n",
    "\n",
    "        # Store query history\n",
    "        self.history.append({\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'sources': sources,\n",
    "            'summary': summary\n",
    "        })\n",
    "\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer_with_citations,\n",
    "            'sources': sources,\n",
    "            'summary': summary,\n",
    "            'history': self.history\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "adv_rag = AdvancedRAGPipeline(rag_retriever, chat_model)\n",
    "result = adv_rag.query(\"what is deep learning is all you need\", top_k=3, min_score=0.1, stream=True, summarize=True)\n",
    "print(\"\\nFinal Answer:\", result['answer'])\n",
    "print(\"Summary:\", result['summary'])\n",
    "print(\"History:\", result['history'][-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGwithPython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
